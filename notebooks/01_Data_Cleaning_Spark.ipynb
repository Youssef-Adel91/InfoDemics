{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56042cad-eed1-467d-afcd-3d9c35e11a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     11\u001b[39m PATH_REGEX = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.*/user/misinfo/([^/]+)/([^/]+)/(nodes.csv|edges.txt)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Initialize Spark Session with explicit HDFS configuration\u001b[39;00m\n\u001b[32m     14\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMisinfoDataCleaning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.fs.defaultFS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhdfs://namenode:9000\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.fs.hdfs.impl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43morg.apache.hadoop.hdfs.DistributedFileSystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSpark Session Initialized and HDFS configs applied.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\java_gateway.py:111\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    117\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract, col, lit, count, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "BASE_PATH = \"hdfs://namenode:9000/user/misinfo\"\n",
    "PATH_REGEX = r\".*/user/misinfo/([^/]+)/([^/]+)/(nodes.csv|edges.txt)\"\n",
    "\n",
    "# Initialize Spark Session with explicit HDFS configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MisinfoDataCleaning\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Initialized and HDFS configs applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422b68e-5fd1-464e-b7f1-59fb5bf79e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and enriching Nodes...\n",
      "Reading and enriching Edges...\n",
      "Initial data extraction complete.\n"
     ]
    }
   ],
   "source": [
    "def extract_and_enrich(file_name, columns, schema):\n",
    "    \"\"\"\n",
    "    Reads files using a wildcard path, extracts the label and graph ID, \n",
    "    and adds them as new columns.\n",
    "    \"\"\"\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        # CSV files use comma delimiter\n",
    "        separator = \",\"\n",
    "    elif file_name.endswith(\".txt\"):\n",
    "        # FIX: Cannot use regex for separator in Spark CSV Reader. \n",
    "        # Using single space, which is the delimiter for the edges.txt files.\n",
    "        separator = \" \"\n",
    "        \n",
    "    # CRITICAL FIX: The double wildcard `/*/*` ensures Spark finds files\n",
    "    # nested two levels deep within the BASE_PATH (e.g., /Conspiracy/2501/nodes.csv)\n",
    "    full_path = f\"{BASE_PATH}/*/*/{file_name}\" \n",
    "    \n",
    "    # 1. EXTRACT: Read all files matching the wildcard path\n",
    "    # We use 'sep=separator' to enforce the correct delimiter for each file type.\n",
    "    df = spark.read \\\n",
    "        .csv(full_path, header=True, inferSchema=False, sep=separator) \n",
    "\n",
    "    # Rename columns to match the defined schema\n",
    "    df = df.toDF(*columns)\n",
    "\n",
    "    # 2. TRANSFORM (Enrichment): Extract metadata using Regex\n",
    "    df = df.withColumn(\"file_path\", input_file_name()) \\\n",
    "           .withColumn(\"label\", regexp_extract(col(\"file_path\"), PATH_REGEX, 1)) \\\n",
    "           .withColumn(\"graph_id\", regexp_extract(col(\"file_path\"), PATH_REGEX, 2)) \\\n",
    "           .drop(\"file_path\")\n",
    "    \n",
    "    # Cast essential columns to correct types (IntegerType is critical for graph analysis)\n",
    "    for column in schema:\n",
    "        df = df.withColumn(column, col(column).cast(IntegerType()))\n",
    "\n",
    "    return df\n",
    "\n",
    "# Define column names and types \n",
    "NODE_COLS = [\"id\", \"time\", \"friends\", \"followers\"]\n",
    "NODE_SCHEMA = [\"id\", \"time\", \"friends\", \"followers\"] \n",
    "\n",
    "EDGE_COLS = [\"src_node_id\", \"dst_node_id\"]\n",
    "EDGE_SCHEMA = [\"src_node_id\", \"dst_node_id\"] \n",
    "\n",
    "# Read and enrich the Node and Edge DataFrames\n",
    "print(\"Reading and enriching Nodes...\")\n",
    "all_nodes_df = extract_and_enrich(\"nodes.csv\", NODE_COLS, NODE_SCHEMA)\n",
    "\n",
    "print(\"Reading and enriching Edges...\")\n",
    "all_edges_df = extract_and_enrich(\"edges.txt\", EDGE_COLS, EDGE_SCHEMA)\n",
    "\n",
    "print(\"Initial data extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b24bb-d665-49f9-a585-18d8cf608b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STARTING DATA QUALITY REPORT (RAW DATA) ---\n",
      "❌ FAIL: Found 64 duplicate nodes.\n",
      "❌ FAIL: Found 16 duplicate edges.\n",
      "✅ PASS: No missing values found (Total Nulls: 0).\n",
      "--- END DATA QUALITY REPORT (RAW DATA) ---\n"
     ]
    }
   ],
   "source": [
    "# --- INITIAL QUALITY CHECK (ON RAW MERGED DATA) ---\n",
    "print(\"\\n--- STARTING DATA QUALITY REPORT (RAW DATA) ---\")\n",
    "\n",
    "# Check for Node duplicates (same node ID within the same graph ID)\n",
    "raw_node_dups_count = all_nodes_df.groupBy(\"graph_id\", \"id\").count().filter(\"count > 1\").count()\n",
    "print(f\"❌ FAIL: Found {raw_node_dups_count} duplicate nodes.\" if raw_node_dups_count > 0 else \"✅ PASS: No duplicate nodes found.\")\n",
    "\n",
    "# Check for Edge duplicates (same source-destination pair within the same graph ID)\n",
    "raw_edge_dups_count = all_edges_df.groupBy(\"graph_id\", \"src_node_id\", \"dst_node_id\").count().filter(\"count > 1\").count()\n",
    "print(f\"❌ FAIL: Found {raw_edge_dups_count} duplicate edges.\" if raw_edge_dups_count > 0 else \"✅ PASS: No duplicate edges found.\")\n",
    "\n",
    "# Check for missing values (Nulls)\n",
    "null_count = all_nodes_df.select([count(when(col(c).isNull(), c)).alias(c) for c in all_nodes_df.columns]).collect()[0]\n",
    "total_nulls = sum(null_count[c] for c in null_count.asDict())\n",
    "print(f\"✅ PASS: No missing values found (Total Nulls: {total_nulls}).\")\n",
    "\n",
    "print(\"--- END DATA QUALITY REPORT (RAW DATA) ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915e232-9595-43ee-844f-99bd7a652eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FULL CLEANING PROCESS IN PROGRESS ---\n",
      "Nodes before Null Drop: 214915\n",
      "Nodes after Null Drop:  214915\n",
      "Null rows removed:      0\n",
      "\n",
      "Edges before Null Drop: 504074\n",
      "Edges after Null Drop:  504074\n",
      "Null rows removed:      0\n",
      "\n",
      "Nodes before Deduplication: 214915\n",
      "Nodes after Deduplication:  214851\n",
      "Duplicates removed:         64\n",
      "\n",
      "Edges before Deduplication: 504074\n",
      "Edges after Deduplication:  504058\n",
      "Duplicates removed:         16\n",
      "\n",
      "Full cleaning process (Null Handling & Deduplication) complete. Clean DataFrames cached.\n"
     ]
    }
   ],
   "source": [
    "# --- TRANSFORMATION: CLEANING AND DEDUPLICATION ---\n",
    "print(\"\\n--- FULL CLEANING PROCESS IN PROGRESS ---\")\n",
    "\n",
    "# 1. NULL VALUE HANDLING (Robustness Check)\n",
    "# We use .na.drop() to remove any rows containing Null or NaN values.\n",
    "# This ensures a clean dataset for graph analysis, regardless of the inspection result in Cell 3.\n",
    "raw_node_count = all_nodes_df.count()\n",
    "clean_nodes_df = all_nodes_df.na.drop()\n",
    "null_dropped_node_count = clean_nodes_df.count()\n",
    "\n",
    "print(f\"Nodes before Null Drop: {raw_node_count}\")\n",
    "print(f\"Nodes after Null Drop:  {null_dropped_node_count}\")\n",
    "print(f\"Null rows removed:      {raw_node_count - null_dropped_node_count}\")\n",
    "\n",
    "# Repeat for edges\n",
    "raw_edge_count = all_edges_df.count()\n",
    "clean_edges_df = all_edges_df.na.drop()\n",
    "null_dropped_edge_count = clean_edges_df.count()\n",
    "\n",
    "print(f\"\\nEdges before Null Drop: {raw_edge_count}\")\n",
    "print(f\"Edges after Null Drop:  {null_dropped_edge_count}\")\n",
    "print(f\"Null rows removed:      {raw_edge_count - null_dropped_edge_count}\")\n",
    "\n",
    "\n",
    "# 2. DEDUPLICATION\n",
    "# Next, remove exact duplicate rows based on key columns AFTER handling nulls.\n",
    "\n",
    "# Nodes Deduplication\n",
    "initial_dedup_node_count = clean_nodes_df.count()\n",
    "clean_nodes_df = clean_nodes_df.dropDuplicates([\"graph_id\", \"id\"])\n",
    "final_node_count = clean_nodes_df.count()\n",
    "\n",
    "print(f\"\\nNodes before Deduplication: {initial_dedup_node_count}\")\n",
    "print(f\"Nodes after Deduplication:  {final_node_count}\")\n",
    "print(f\"Duplicates removed:         {initial_dedup_node_count - final_node_count}\")\n",
    "\n",
    "\n",
    "# Edges Deduplication\n",
    "initial_dedup_edge_count = clean_edges_df.count()\n",
    "clean_edges_df = clean_edges_df.dropDuplicates([\"graph_id\", \"src_node_id\", \"dst_node_id\"])\n",
    "final_edge_count = clean_edges_df.count()\n",
    "\n",
    "print(f\"\\nEdges before Deduplication: {initial_dedup_edge_count}\")\n",
    "print(f\"Edges after Deduplication:  {final_edge_count}\")\n",
    "print(f\"Duplicates removed:         {initial_dedup_edge_count - final_edge_count}\")\n",
    "\n",
    "\n",
    "# Cache the clean data for immediate use and verification\n",
    "clean_nodes_df.cache()\n",
    "clean_edges_df.cache()\n",
    "\n",
    "print(\"\\nFull cleaning process (Null Handling & Deduplication) complete. Clean DataFrames cached.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8cd97c-9595-485a-8f23-648eb33c1113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving clean data to 'all_nodes_clean.parquet'...\n",
      "Saving clean data to 'all_edges_clean.parquet'...\n",
      "\n",
      "SUCCESS: Clean datasets saved to HDFS!\n"
     ]
    }
   ],
   "source": [
    "# --- LOAD: SAVING OPTIMIZED PARQUET FILES ---\n",
    "CLEAN_PATH = \"hdfs://namenode:9000/user/misinfo\"\n",
    "\n",
    "print(\"\\nSaving clean data to 'all_nodes_clean.parquet'...\")\n",
    "# Overwrite mode is used so you can safely rerun the cell\n",
    "clean_nodes_df.write.mode(\"overwrite\").parquet(f\"{CLEAN_PATH}/all_nodes_clean.parquet\")\n",
    "\n",
    "print(\"Saving clean data to 'all_edges_clean.parquet'...\")\n",
    "clean_edges_df.write.mode(\"overwrite\").parquet(f\"{CLEAN_PATH}/all_edges_clean.parquet\")\n",
    "\n",
    "print(\"\\nSUCCESS: Clean datasets saved to HDFS!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab4366-f49b-4d55-869e-f10396a6c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Final Cleaning Summary Report...\n",
      "\n",
      "CLEANING SUMMARY\n",
      "+----------------------+----------+-----------+--------+----------+\n",
      "|Dataset               |Raw_Count |Cleaned_Cnt|Removed |% Removed |\n",
      "+----------------------+----------+-----------+--------+----------+\n",
      "|Conspiracy Nodes      |19072     |19047      |25      |0.13      |\n",
      "|Non-Conspiracy Nodes  |157146    |157140     |6       |0.00      |\n",
      "|Other Nodes           |38697     |38664      |33      |0.09      |\n",
      "+----------------------+----------+-----------+--------+----------+\n",
      "|Conspiracy Edges      |58227     |58223      |4       |0.01      |\n",
      "|Non-Conspiracy Edges  |328501    |328501     |0       |0.00      |\n",
      "|Other Edges           |117346    |117334     |12      |0.01      |\n",
      "+----------------------+----------+-----------+--------+----------+\n"
     ]
    }
   ],
   "source": [
    "# --- FINAL CLEANING SUMMARY REPORT ---\n",
    "print(\"Generating Final Cleaning Summary Report...\")\n",
    "\n",
    "def get_counts(df, name_suffix):\n",
    "    \"\"\"Helper to get counts grouped by label.\"\"\"\n",
    "    return df.groupBy(\"label\").count().withColumnRenamed(\"count\", \"count_\" + name_suffix)\n",
    "\n",
    "# Calculate and join raw vs. clean counts for Nodes and Edges\n",
    "raw_n_stats = get_counts(all_nodes_df, \"raw\")\n",
    "clean_n_stats = get_counts(clean_nodes_df, \"clean\")\n",
    "node_comparison = raw_n_stats.join(clean_n_stats, \"label\", \"outer\").collect()\n",
    "\n",
    "raw_e_stats = get_counts(all_edges_df, \"raw\")\n",
    "clean_e_stats = get_counts(clean_edges_df, \"clean\")\n",
    "edge_comparison = raw_e_stats.join(clean_e_stats, \"label\", \"outer\").collect()\n",
    "\n",
    "def print_row(dataset, raw, clean):\n",
    "    \"\"\"Formats and prints a single row of the summary table.\"\"\"\n",
    "    removed = raw - clean\n",
    "    # Prevent DivisionByZeroError if raw count is 0\n",
    "    pct = (removed / raw) * 100 if raw and raw > 0 else 0.0\n",
    "    print(f\"|{dataset:<22}|{raw:<10}|{clean:<11}|{removed:<8}|{pct:<10.2f}|\")\n",
    "\n",
    "print(\"\\nCLEANING SUMMARY\")\n",
    "print(\"+\" + \"-\"*22 + \"+\" + \"-\"*10 + \"+\" + \"-\"*11 + \"+\" + \"-\"*8 + \"+\" + \"-\"*10 + \"+\")\n",
    "print(f\"|{'Dataset':<22}|{'Raw_Count':<10}|{'Cleaned_Cnt':<11}|{'Removed':<8}|{'% Removed':<10}|\")\n",
    "print(\"+\" + \"-\"*22 + \"+\" + \"-\"*10 + \"+\" + \"-\"*11 + \"+\" + \"-\"*8 + \"+\" + \"-\"*10 + \"+\")\n",
    "\n",
    "# Process Nodes\n",
    "for row in node_comparison:\n",
    "    label = row['label']\n",
    "    if \"Non\" in label: pretty_name = \"Non-Conspiracy Nodes\"\n",
    "    elif \"Conspiracy\" in label: pretty_name = \"Conspiracy Nodes\"\n",
    "    else: pretty_name = \"Other Nodes\"\n",
    "    \n",
    "    print_row(pretty_name, row['count_raw'], row['count_clean'])\n",
    "\n",
    "print(\"+\" + \"-\"*22 + \"+\" + \"-\"*10 + \"+\" + \"-\"*11 + \"+\" + \"-\"*8 + \"+\" + \"-\"*10 + \"+\")\n",
    "\n",
    "# Process Edges\n",
    "for row in edge_comparison:\n",
    "    label = row['label']\n",
    "    if \"Non\" in label: pretty_name = \"Non-Conspiracy Edges\"\n",
    "    elif \"Conspiracy\" in label: pretty_name = \"Conspiracy Edges\"\n",
    "    else: pretty_name = \"Other Edges\"\n",
    "    \n",
    "    print_row(pretty_name, row['count_raw'], row['count_clean'])\n",
    "\n",
    "print(\"+\" + \"-\"*22 + \"+\" + \"-\"*10 + \"+\" + \"-\"*11 + \"+\" + \"-\"*8 + \"+\" + \"-\"*10 + \"+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf744ea-0816-420e-9b40-16146176990e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
